<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CKA考试经验分享]]></title>
    <url>%2F2019%2F08%2F03%2FCKA-%E8%80%83%E8%AF%95%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[cka 考试体验分享一，考试简介​ CKA是Google CNCF云原生基金会和Linux 基金会官方2017年推出的Kubernetes管理员认证计划，也是目前官方唯一的云原生认证，用于证明持有人具备履行Kubernetes管理的知识、技能等相关的能力，认证是这一过程中的关键步骤，顺利通过认证的管理员和企业，证明在开源技术体系中有能力快速建立自己的信誉和价值。CKA认证考试题目全部为模拟环境的实际操作，全英文一对一监考，题目多操作难度大，时长3小时非常紧张，通过率偏低。 二，考纲三，考试说明​ CKA考试时长为三小时，分数总计为100分，74分为及格线，总体来说三个小时时间还是挺紧张的。考试方式可以选择线上与线下，目前CNCF正在中国内地考点试点，未来几大一线城市都将会设置考点。笔者正是参与了CKA内地试点beta阶段的深圳线下考试名额，考试费用打五折。 四，试题解答与分享1.列出环境内所有的pv 并以 name字段排序（使用kubectl自带排序功能） kubectl get pv --sort-by=&#39;.metadata.name&#39; 2.列出指定pod的日志中状态为Error的行，并记录在指定的文件上kubectl logs $podname -n $namespace | grep error 3.列出k8s可用的节点，不包含不可调度的 和 NoReachable的节点，并把数字写入到文件里（1）kubectl get node (笨办法，数出来)（2）定义变量，利用kubectl 自带的参数查找出当前节点个数 ​ JSONPATH=&#39;{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}= {@.status};{end}{end}&#39; \​ kubectl get nodes -o jsonpath=&quot;$JSONPATH&quot; | grep &quot;Ready=True&quot;4.创建一个pod名称为nginx，并将其调度到节点为 disk=ssd上kubectl run nginx --image=nginx --restart=Never -o yaml --dry-run &gt; 4.yaml修改对应的yaml文件 123456789101112131415apiVersion: v1kind: Podmetadata: creationTimestamp: null labels: run: nginx name: nginxspec: containers: - image: nginx name: nginx nodeSelector: disk: stat dnsPolicy: ClusterFirst restartPolicy: Never 5.提供一个pod的yaml，要求添加Init Container，Init Container的作用是创建一个空文件，pod的Containers判断文件是否存在，不存在则退出 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: creationTimestamp: null labels:run: nginx name: nginx-5spec: initContainers: - name: init-nginx image: nginx command: ['sh', '-c', 'touch /opt/file1'] volumeMounts: - name: workdir mountPath: /opt containers: - image: nginx name: nginx command: ['sh', '-c', 'if [ -f /opt/file1 ];then sleep 3600h;fi'] volumeMounts: - name: workdir mountPath: /opt volumes: - name: workdir emptyDir: &#123;&#125; dnsPolicy: ClusterFirst restartPolicy: Never 6.指定在命名空间内创建一个pod名称为test，内含四个指定的镜像nginx、redis、memcached、busybox 123456789101112131415apiVersion: v1kind: Podmetadata: labels:run: nginx name: nginx-6spec: containers: - image: nginx name: nginx - image: busybox name: busybox command: ['sh', '-c', 'sleep 3600h'] dnsPolicy: ClusterFirst restartPolicy: Never 7.创建一个pod名称为test，镜像为nginx，Volume名称cache-volume为挂在在/data目录下，且Volume是non-Persistent的 12345678910111213141516171819apiVersion: v1kind: Podmetadata: creationTimestamp: null labels:run: nginx name: nginx-7spec: containers: - image: nginx name: nginx volumeMounts: - name: cache-volume mountPath: /data volumes: - name: cache-volume emptyDir: &#123;&#125; dnsPolicy: ClusterFirst restartPolicy: Never 8.列出Service名为test下的pod 并找出使用CPU使用率最高的一个，将pod名称写入文件中 (1)先找出svc下的pod的标签 kubectl get svc test -o wide （2）使用kubectl top命令找到cpu使用率最高的一个写入到对应的文件中 kubectl top pod -l app:frontend 9.创建一个Pod名称为nginx-app，镜像为nginx，并根据pod创建名为nginx-app的Service，type为NodePortkubectl create svc nodeport nginx-app --tcp=8080:80 --dry-run -o yaml &gt; 10.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: labels: run: nginx-app name: nginx-appspec: ports: - name: 8080-80 port: 8080 protocol: TCP targetPort: 80 selector: run: nginx-app type: NodePort 10.创建一个nginx的Workload，保证其在每个节点上运行，注意不要覆盖节点原有的Tolerations 12345678910111213141516171819202122apiVersion: apps/v1kind: DaemonSetmetadata: labels:run: nginx name: nginxspec: selector: matchLabels: run: nginx template: metadata: labels: run: nginx spec: containers: - image: nginx name: nginx-daemonset tolerations: - key: foo value: bar effect: NoSchedule 11.将deployment为nginx-app的副本数从1变成4kubectl scale deployment nginx-app --replicas=412.创建nginx-app的deployment ，使用镜像为nginx:1.11.0-alpine ,修改镜像为1.11.3-alpine，并记录升级，再使用回滚，将镜像回滚至nginx:1.11.0-alpinekubectl set image deploy nginx-app nginx-app=nginx:1.11kubectl rollout undo deploy nginx-app13.根据已有的一个nginx的pod、创建名为nginx的svc、并使用nslookup查找出service dns记录，pod的dns记录并分别写入到指定的文件中 123456789101112[ root@curl:/ ]$ nslookup nginx-appServer: 10.10.0.10Address 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.localName: nginx-appAddress 1: 10.10.197.20 nginx-app.default.svc.cluster.local[ root@curl:/ ]$ nslookup 10-20-2-62.default.pod.cluster.localServer: 10.10.0.10Address 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.localName: 10-20-2-62.default.pod.cluster.localAddress 1: 10.20.2.62 14.创建Secret 名为mysecret，内含有password字段，值为blake，然后 在pod1里 使用ENV进行调用，Pod2里使用Volume挂载在/data 下 kubectl create secret my-secret –from-literal-key:password=blake 1234567891011121314151617181920212223242526272829303132333435[root@ECB-LPY21xx ~]# cat 14-1.yaml apiVersion: v1kind: Podmetadata: labels: run: nginx name: nginx-env-secretspec: containers: - image: nginx name: nginx-env-secret env: - name: SECRET_PASSWD valueFrom: secretKeyRef: key: passwd name: mysecret[root@ECB-LPY21067854 ~]# cat 14-2.yaml apiVersion: v1kind: Podmetadata: labels: run: nginx name: nginx-volume-secretspec: containers: - image: nginx name: nginx-volume-secret volumeMounts: - name: nginx-volume-secret mountPath: /data volumes: - name: nginx-volume-secret secret: secretName: mysecret 15.使node1节点不可调度，并重新分配该节点上的podkubectl drain node01 --ignore-daemonsets --delete-local-data 16.使用etcd 备份功能备份etcd（提供enpoints，ca、cert、key）ETCD_API=3 etcdctl --cert-file= --key-file= --ca-file= --endpoints snapshot save 17.给出一个失联节点的集群，排查节点故障，要保证改动是永久的。systemctl status kubelet &amp;&amp; systemctl start enable kubelet18.给出一个集群，排查出集群的故障 kubectl cluster-info 最终排查出是集群master的节点kubelet的静态文件的目录错了 19.给出一个节点，完善kubelet配置文件，要求使用systemd配置kubelet ​ 主要涉及kubelet的配置的修改，修改静态文件的目录 20.给出一个集群，将节点node1添加到集群中，并使用TLS bootstrapping 21.创建一个pv，类型是hostPath，位于/data中，大小1G，模式ReadOnlyMany 12345678910111213apiVersion: v1kind: PersistentVolumemetadata: name: pv0005spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /data]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下数据无损动态修改硬盘MBR分区表为GPT]]></title>
    <url>%2F2019%2F04%2F04%2FLinux-%E4%B8%8B%E6%95%B0%E6%8D%AE%E6%97%A0%E6%8D%9F%E5%8A%A8%E6%80%81%E4%BF%AE%E6%94%B9%E7%A1%AC%E7%9B%98MBR%E5%88%86%E5%8C%BA%E8%A1%A8%E4%B8%BAGPT%2F</url>
    <content type="text"><![CDATA[1.简介 腾讯云官网中有一篇帮助文档“扩容Linux文件系统，https://www.qcloud.com/document/product/362/6738，讲解了GPT分区云硬盘扩容后修改分区指引和MBR分区云硬盘扩容后修改分区指引。其中MBR分区扩容下，若扩容后的空间已经大于2TB则不可选择。官网文档没有涉及到MBR的分区扩展到2T以上该怎么处理。 注：写在文前。不管任何形式的扩容硬盘，最好都事先做好硬盘数据的快照，避免手误或者其他因素造成数据丢失。数据无价！！！ 2.下面讲解数据无损动态调整MBR的分区为GPT（1）fdisk -l /dev/vdb &amp;&amp; partprobe -s （2）下面以/dev/vdc这块盘做讲解，新建了一个/dev/vdc1分区，并且格式化为ext3文件系统。 fdsik -l /dev/vdc &amp;&amp; mkfs.ext3 /dev/vdc1 （3）下面我在控制台把/dev/vdc这块硬盘扩容到2T以上，并且重新挂载。 （4）接下来看一下，在MBR分区表下，扩容/dev/vdc1到2.5T能不能行？ 这里看到只能扩容到2TB。gg （5）显然这是MBR分区表的局限。MBR分区表共512个字节。前446字节包括boot loader信息和grub引导信息，还有64字节的磁盘分区信息，以及2个字节的结束标志。而每个主分区要在分区表里占用16字节。所以单个硬盘只能分64/16个主分区。至于为什么每个分区不能超过2TB，请看下图。 （6）接下来说一下我这个数据无损在线动态调整MBR为GPT方法吧。注：不管是怎么扩容硬盘，扩容前都建议对云硬盘做一下快照。 A.为了证明数据真的无损，我们新建几个file。最后再来看一下数据情况。 B.卸载/dev/vdc1 C.用gdisk(安装方法:yum install gdisk)把MBR分区格式转换程GPT分区格式(MBR不支持2T以上大小) 注：这个方法在大部分场景下都是可以转mbr为gpt的，只有磁盘开头前33个扇区，或最后34个扇区被分区占用的场景不支持。我们的硬盘分区时，默认是从2048扇区开始分的，所以一般不会出现前34个扇区被占用，客户控制台扩容实体云硬盘之后，后33扇区也不存在被占用。所以绝大多数情况下前34后33分区都不会被占用。除非客户主动刻意去占用。这里可以通过part ed 硬盘–unit s–p查看硬盘前34分区有没有被占用。 D.使用parted删除vdc1，并重新创建vdc1(fdisk不支持超过2TB大小，注意记住删除前vdc1的start sector，创建新的vdc1的时候start sector必须和删除前一致) E.目前还需要扩展下分区，才能挂载正常显示分区大小。 注：这里也可能会遇到下面这种情况 ‘’’ [root@bobo ~]# e2fsck -yf /dev/vdc1 e2fsck 1.42.9 (28-Dec-2013) The filesystem size (according to the superblock) is xxx blocks The physical size of the device is xxx blocks Either the superblock or the partition table is likely to be corrupt! Abort? yes ‘’’ 这里是检测到分区表变化了，询问是否放弃修复，并不是报错。这里我加y这个参数的原因在于，如果出现错误直接fix。如果遇到了这种情况，可以直接用这个命令e2fsck -f /dev/vdb1直接修复分区就ok。 F.重新挂载下看下分区大小,并检查下数据完整性 3.GPT分区的优越性（GUID partition table, GPT 磁盘分区表。）1因为过去一个扇区大小就是 512Bytes 而已，不过目前已经有 4K 的扇区设计出现！为了相容于所有的磁盘，因此在扇区的定义上面， 大多会使用所谓的逻辑区块位址（Logical Block Address, LBA）来处理。GPT 将磁盘所有区块以此 LBA（默认为 512Bytes ！） 来规划，而第一个 LBA 称为 LBA0 （从 0 开始编号）。与 MBR 仅使用第一个 512Bytes 区块来纪录不同， GPT 使用了 34 个 LBA 区块来纪录分区信息！同时与过去 MBR 仅有一的区块，被干掉就死光光的情况不同， GPT 除了前面 34 个 LBA 之外，整个磁盘的最后 33 个 LBA 也拿来作为另一个备份！这样或许会比较安全些吧！ 结构详解： LBA0：和传统MBR分区一样，仍然为主引导记录 LBA1：我们称之为“主分区头” LBA2-33：共计32个扇区，我们称之为“主分区节点” LBA-1：我们称之为“备份分区头”，它就是“主分区头”的一个Copy LBA-2-33：共计32个扇区，我们称之为“备份分区节点”，它就是“主分区节点”的一个Copy LBA34：正常的GPT分区内容，文件系统（如：FAT，NTFS，EXT等）就是构建在这里面。 大概了解一下:LBA2-LBA33 4.常用的分区处理的工具（1）fdisk(MBR) （2）gdisk(GPT) （3）parted（MBR和GPT通用） 注:gdisk和fdisk最好不要混用，不要用fdsik处理gpt分区，也不要用gdisk处理mbr分区，一不小心，数据搞没了，后悔都来不及。 云+社区链接：https://cloud.tencent.com/developer/article/1175328]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MBR</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 升级系统内核]]></title>
    <url>%2F2019%2F02%2F28%2FCentos7-%E5%8D%87%E7%BA%A7%E7%B3%BB%E7%BB%9F%E5%86%85%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[Centos7 升级系统内核 安装yum源 12rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 查看列表 12yum --disablerepo=* --enablerepo=elrepo-kernel repolistyum --disablerepo=* --enablerepo=elrepo-kernel list kernel* 安装最新版本的kernel 1yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y 设置为默认内核 12345[root@localhost ~]# awk -F\&apos; &apos;$1==&quot;menuentry &quot; &#123;print $2&#125;&apos; /etc/grub2.cfgCentOS Linux (4.20.13-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-54dbc7f0d8ee49babd50ed6f840123fa) 7 (Core)当前默认内核为1 12345678910[root@localhost ~]# vim /etc/default/grub GRUB_TIMEOUT=5GRUB_DISTRIBUTOR=&quot;$(sed &apos;s, release .*$,,g&apos; /etc/system-release)&quot;GRUB_DEFAULT=0GRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT=&quot;console&quot;GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet&quot;GRUB_DISABLE_RECOVERY=&quot;true&quot;GRUB_DEFAULT=saved 改成 GRUB_DEFAULT=0 12[root@localhost ~]# grub2-mkconfig -o /boot/grub2/grub.cfg生成新配置文件 重启系统查看内核 123[root@localhost ~]# reboot[root@localhost ~]# uname -r4.20.13-1.el7.elrepo.x86_64]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM虚拟化技术之使用Qemu-kvm创建和管理虚拟机]]></title>
    <url>%2F2019%2F02%2F27%2FKVM%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B9%8B%E4%BD%BF%E7%94%A8Qemu-kvm%E5%88%9B%E5%BB%BA%E5%92%8C%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[KVM虚拟化技术之使用Qemu-kvm创建和管理虚拟机 KVM介绍 KVM是开源软件，全称是kernel-based virtual machine（基于内核的虚拟机），属于内核的一个模块，Linux 2.6.20核心以上的版本中默认带有kvm模块。它包含一个为处理器提供底层虚拟化 可加载的核心模块kvm.ko（kvm-intel.ko或kvm-AMD.ko） kvm虚拟机=kvm模块 + qemu模拟器 kvm负责分配内存和cpu，qemu负责模拟网络设备和io设备 系统初始化 检查服务器是否支持虚拟化 1[root@kvm ~]# egrep &apos;(vmx|svm)&apos; /proc/cpuinfo 检查内核中是否加载kvm模块 1234[root@kvm ~]# lsmod | grep kvmkvm_intel 174841 3 kvm 578518 1 kvm_intelirqbypass 13503 3 kvm 关闭selinux 12345[root@kvm ~]# cat /etc/sysconfig/selinux | grep &quot;SELINUX&quot;# SELINUX= can take one of these three values:SELINUX=disabled# SELINUXTYPE= can take one of three two values:SELINUXTYPE=targeted 为KVM虚拟机配置桥接网络 新建网桥br0，并配置，转移ip到网桥上 1234567891011[root@kvm ~]# cd /etc/sysconfig/network-scripts/[root@kvm network-scripts]# cat ifcfg-br0 TYPE=BridgeBOOTPROTO=noneDEFROUTE=yesNAME=br0DEVICE=br0ONBOOT=yesIPADDR=192.168.175.6PREFIX=24GATEWAY=192.168.175.2 配置eth0使用桥接模式 1234567[root@kvm network-scripts]# cat ifcfg-eth0 TYPE=EthernetBOOTPROTO=noneNAME=eth0DEVICE=eth0ONBOOT=yesBRIDGE=br0 配置完成后，重启网络服务 1[root@kvm network-scripts]# systemctl restart network 查看ifconfig如下 1234567891011121314151617[root@kvm network-scripts]# ifconfig br0br0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.175.6 netmask 255.255.255.0 broadcast 192.168.175.255 inet6 fe80::20c:29ff:fe52:20f6 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:52:20:f6 txqueuelen 1000 (Ethernet) RX packets 49 bytes 4628 (4.5 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 53 bytes 7133 (6.9 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@kvm network-scripts]# ifconfig eth0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 ether 00:0c:29:52:20:f6 txqueuelen 1000 (Ethernet) RX packets 6382 bytes 469272 (458.2 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8765 bytes 703077 (686.5 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 安装kvm相关组件 安装命令 1[root@kvm network-scripts]# yum install qemu-kvm python-virtinst virt-* libvirt libvirt-client bridge-utils qemu-img -y 各组件功能 kvm模块：qemu-kvm 图形界面管理虚拟机：virt-manager 网络接口管理工具：bridge-utils 虚拟机管理工具：libvirt 虚拟机管理工具客户端：libivirt-client python组件，记录xml信息：python-virtinst qemu组件,创建硬盘，启动虚拟机等：qemu-img 虚拟机安装命令：virt-install 启动libvirt服务 1[root@kvm network-scripts]# systemctl start libvirtd &amp;&amp; systemctl enable libvirtd &amp;&amp; systemctl status libvirt 查看系统网络，会自动生成一个桥设备，默认虚拟机和宿主机通信的设备 123456789101112[root@kvm network-scripts]# ifconfig virbr0virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:66:6f:6a txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@kvm network-scripts]# brctl showbridge name bridge id STP enabled interfacesbr0 8000.000c295220f6 no eth0virbr0 8000.525400666f6a yes virbr0-nic 安装虚拟机 创建硬盘设备 1qemu-img create -f qcow2 /data/vm1.qcow2 5G 创建虚拟机 12virt-install --virt-type kvm --name vm2 --ram 512 --cdrom=/data/CentOS-7.5-x86_64-DVD-1804.iso --disk path=/data/vm1.qcow2 --network bridge=br0 --graphics vnc,listen=0.0.0.0 --noautoconsole注：自行上传iso系统镜像 安装tigervnc或者vnc viewe工具可以连接到安装虚拟机图形界面，默认端口5900 查看kvm进程 1234[root@kvm ~]# ps -ef | grep kvmroot 628 2 0 22:02 ? 00:00:00 [kvm-irqfd-clean]avahi 714 1 0 22:02 ? 00:00:00 avahi-daemon: running [kvm.local]qemu 3070 1 3 22:21 ? 00:01:26 /usr/libexec/qemu-kvm -name vm2 -S -machine pc-i440fx-rhel7.0.0,accelkvm,usb=off,dump-guest-core=off -cpu IvyBridge-IBRS -m 512 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid ef1dcbd8-1684-4ae0-9dc7-500d59236911 -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-2-vm2/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=delay -no-hpet -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x5 -drive file=/data/vm1.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive if=none,id=drive-ide0-0-0,readonly=on -device ide-cd,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=26,id=hostnet0,vhost=on,vhostfd=28 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:0f:1f:00,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,path=/var/lib/libvirt/qemu/channel/target/domain-2-vm2/org.qemu.guest_agent.0,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 -device usb-tablet,id=input0,bus=usb.0,port=1 -vnc 0.0.0.0:0 -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x7 -msg timestamp=on 实际上一个虚拟机对于宿主机而言，只是宿主机上的一个进程而已 查看虚拟机 1234[root@kvm ~]# virsh list Id Name State---------------------------------------------------- 2 vm2 running virsh常用命令 1234567virsh list --all ##列出所有虚拟机virsh list ##列出运行中的虚拟机virsh start vm ##启动虚拟机virsh shutdown vm ##关闭虚拟机virsh undfine vm ##销毁虚拟机virsh console vm1 ##通过console连接虚拟机virsh edit vm1 ##编辑位于/etc/libvirt/qemu/vm1.xml]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>kvm</tag>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F02%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
